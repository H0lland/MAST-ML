[General Setup]
	save_path = /path/to/save/mastmlrun			# Fill in with path to save your MASTML run in
	input_features = Auto					# List features exactly as they appear in data file, or “Auto” to extract all
	target_feature = y_feature_regression			# Fill in with name of y_feature in data file you’re fitting to. Need “regression” at end.
    grouping_feature = grouping_name            # Optional, set with labeling_features (set to None if not using); must specify as same grouping name in input data file column
    labeling_features = x_feature1, x_feature2  # Optional, set with grouping_feature (set to None if not using); specify one or more particular x_feature names for explicit labeling

[Data Setup]

	[[Initial]]
		data_path = /path/to/datafile.csv		# Fill in with path to data file. Accepted extensions: .csv, .xls, .xlsx

[Feature Normalization]

	normalize_x_features = False 				# True or False, whether to normalize x feature values
	normalize_y_feature = False 				# True or False, whether to normalize y feature values
	feature_normalization_type = standardize 		# Choose “standardize” or “normalize”, method of data normalization
	feature_scale_min = 0					# Only used if “normalize” selected above. The minimum value of normalized data
	feature_scale_max = 1					# Only used if “normalize” selected above. The maximum value of normalized data

[Feature Generation]

	perform_feature_generation = False			# True or False, whether to perform feature generation
	add_magpie_features = False				# True or False, whether to construct features from elemental descriptors
	add_materialsproject_features = False			# True or False, whether to construct features from data on materialsproject.org
	materialsproject_apikey = '' 				# Your Materials Project API key. Get your key at https://materialsproject.org/janrain/loginpage/?next=/dashboard
	add_citrine_features = True				# True or False, whether to construct features from data on citrination.com
	citrine_apikey = '' 					# Your Citrination API key. Get your key at https://citrination.com/users/edit

[Feature Selection]

	perform_feature_selection = False			# True or False, whether to perform feature selection
	remove_constant_features = False			# True or False, whether to remove features that have the same value for all input data
	feature_selection_algorithm = basic_forward_selection	# Type of feature selection algorithm to use. Choose from “basic_forward_selection”, “sequential_forward_selection”, “univariate_feature_selection”, “recursive_feature_elimination”
	use_mutual_information = False				# True or False, whether to use mutual information between features to guide feature selection. Only used in univariate feature selection.
	number_of_features_to_keep = 10				# Number of features to keep after feature selection. If a number larger than the total feature amount is given, all features will be kept.
	scoring_metric = root_mean_squared_error		# Scoring metric to use in feature selection. Currently only impacts univariate feature selection and recursive feature elimination. Choose from “mean_squared_error”, “mean_absolute_error”, “root_mean_squared_error” and “r2_score”.
	generate_feature_learning_curve = False			# True or False, whether to generate a feature learning curve. Currently only impacts univariate feature selection and recursive feature elimination.
	model_to_use_for_learning_curve = gkrr_model_regressor	# Model type to use in constructing learning curve. Currently only impacts univariate feature selection and recursive feature elimination.

[Models and Tests to Run]

	models = linear_model_regressor, gkrr_model_regressor	# List of methods to construct machine learning models. If running multiple models, separate model keywords by commas as shown. For complete list of available models, see Model Parameters section below and the documentation.
	test_cases = SingleFit, KFoldCV				# List of tests to run for each model listed above. If running multiple tests, separate test keywords by commas as shown. For complete list of available tests, see Test Parameters section below and see the documentation.

[Test Parameters]						# Each type of test and its associated parameters are listed here. Only the tests listed above under Models and Tests to Run will be run. For more info, see the MASTML documentation

	[[SingleFit]]

		training_dataset = Initial
		testing_dataset  = Initial
		xlabel = Xdata
		ylabel = ydata
		plot_filter_out = None

	[[SingleFitPerGroup]]

		training_dataset = Initial
		testing_dataset  = Initial
		xlabel = Xdata
		ylabel = ydata
		plot_filter_out = None

	[[SingleFitGrouped]]

		training_dataset = Initial
		testing_dataset  = Initial
		xlabel = Xdata
		ylabel = ydata
		plot_filter_out = None

	[[KFoldCV]]

		training_dataset = Initial
		testing_dataset  = Initial
		xlabel = Xdata
		ylabel = ydata
		num_folds = 5
		num_cvtests = 10
		mark_outlying_points = 0

	[[LeaveOutPercentCV]]

		training_dataset = Initial
		testing_dataset  = Initial
		xlabel = Xdata
		ylabel = ydata
		percent_leave_out = 20
		num_cvtests = 10

	[[LeaveOneOutCV]]

		training_dataset = Initial
		testing_dataset  = Initial
		xlabel = Xdata
		ylabel = ydata
		mark_outlying_points = 0
		num_cvtests = 10

	[[LeaveOutGroupCV]]

		training_dataset = Initial
		testing_dataset  = Initial
		xlabel = Xdata
		ylabel = ydata
		mark_outlying_points = 0
		num_cvtests = 10

	[[PredictionVsFeature]]

		training_dataset = Initial
		testing_dataset  = Initial
		xlabel = Xdata
		ylabel = ydata
		plot_filter_out = None
		feature_plot_feature = x_feature
		data_labels = data_name

	[[PlotNoAnalysis]]

		training_dataset = Initial
		testing_dataset  = Initial
		xlabel = Xdata
		ylabel = ydata
		plot_filter_out = None
		feature_plot_feature = x_feature
		data_labels = data_name

	[[ParamOptGA]]

		training_dataset = Initial
		testing_dataset = Initial
		num_folds = 5
		num_cvtests = 10
		pop_upper_limit = 1000000
		param_1 = model;C;float;continuous-log;-3:3:50 		# Example is for SVR model
		param_2 = model;gamma;float;continuous-log;-3:3:50	# Example is for SVR model

	[[ParamGridSearch]]

		training_dataset = Initial
		testing_dataset = Initial
		num_folds = 5
		num_cvtests = 5
		pop_upper_limit = 5000
		param_1 = model;alpha;float;continuous-log;-10:0:50 	# Example is for GKRR model
		param_2 = model;gamma;float;continuous-log;-3:3:50 	# Example is for GKRR model


[Model Parameters]						# Each type of model and its associated parameters are listed here. Only the models listed above under Models and Tests to Run will be run. For more info, see the scimitar-learn docs at http://scikit-learn.org/stable/documentation.html

	[[linear_model_regressor]]

		fit_intercept = True

	[[linear_model_lasso_regressor]]

		alpha = 1.0
		fit_intercept = True

	[[gkrr_model_regressor]]

		alpha = 1.0
		gamma = 0.0
		coef0 = 1
		degree = 3
		kernel = rbf

	[[support_vector_machine_model_regressor]]

		error_penalty = 1.0
		kernel = rbf
		degree = 3
		gamma = 0.001
		coef0 = 0.0

	[[decision_tree_model_regressor]]

		criterion = mse
		max_depth = 4
		min_samples_leaf = 1
		min_samples_split = 2
		splitter = best

	[[extra_trees_model_regressor]]

		criterion = mse
		max_depth = 4
		max_leaf_nodes = 3
		min_samples_leaf = 1
		min_samples_split = 2
		n_estimators = 10

	[[randomforest_model_regressor]]

		criterion = mse
		max_depth = 4
		max_leaf_nodes = 3
		min_samples_leaf = 1
		min_samples_split = 2
		n_estimators = 10
		n_jobs = 1
		warm_start = False

	[[adaboost_model_regressor]]

		base_estimator_max_depth = 4
                learning_rate = 1.0
                loss = linearsquare
                n_estimators = 50

	[[nn_model_regressor]]

		activation = relu
		alpha = 0.0001
		hidden_layer_sizes = (100,)
		max_iterations = 200
		solver = adam
		tolerance = 0.0001

	[[gaussianprocess_model_regressor]]

		RBF_length_scale = 1
		RBF_length_scale_bounds = 1
		alpha = 1*10**-10
		kernel = rbf
		n_restarts_optimizer = 0
		normalize_y = True
		optimizer = fmin_l_bfgs_b
